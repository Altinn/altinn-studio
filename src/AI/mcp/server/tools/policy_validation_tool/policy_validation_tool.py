import os
import json
import xml.etree.ElementTree as ET
from typing import Dict, Any, List, Optional
from mcp.types import ToolAnnotations


# Third-party imports
from langchain_openai import AzureChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser

from server.tools import register_tool
from server.tools.policy_tool.static import all_roles
from server.config import LLM_PROMPTS
from server.config import (
    AZURE_ENDPOINT,
    API_KEY,
    DEPLOYMENT_NAME,
    LLM_CONFIG
)


@register_tool(
    name="policy_validation_tool",
    description="""
    This tool provides a validation access control and authorization rules in Altinn applications.
    Use this tool when you want to make sure the right roles has the right accesses accross the application.

    The tool has to inputs:
    1. User query: which accesses does the user want different roles to have
    2. Policy rules summary: summary of all authorization rules in the policy file, generated by policy_summarization_tool

    The output is a JSON object containing the user query, the rules that matches the query, and the rules that does not match the query.
    
    It is crucial that Policy Summarization Tool is run before Policy Validation Tool, to get the the summary of all rules as an input. 
    Policy validation tool and policy summarization tool should be used after every change to the policy.xml file.
""",
    title="Policy Validation Tool",
    annotations=ToolAnnotations(
        title="Policy Validation Tool",
        readOnlyHint=True
    )
)
def policy_validation_tool(query: str, policy_rules: dict | list) -> dict:
    """
    Validates a policy file based on match with the user query.
    A server-side LLM is used to analyze the rules in the policy file and compare all authorization rules with the user query.
    
    Args:
        query: The user query for policy validation
        policy_rules: Summarization of rules in policy, generated by policy_summarization_tool.
                     Can be either a dictionary with a 'rules' key or a direct list of rules.
    
    Returns:
        A dictionary containing the LLM analysis.
        The analaysis contains the user query, the rules that matches the query, and the rules that does not match the query.
    """
    
    #Check if policy rules is provided
    if not policy_rules:
        return {"status": "error", "message": "No policy rules provided. Please specify a policy to validate.", "hint": "Use policy_summarization_tool to generate policy rules."}

    try:
        #Converts policy rules to string
        policy_rules_string = convert_policy_rules_to_string(policy_rules)
        
        # Get LLM analysis of validation results based on user query
        llm_analysis = validate_policy_with_llm(query, policy_rules_string)
        
        # Return both validation results and LLM analysis
        return {
            #"validation_results": validation_results,
            "llm_analysis": llm_analysis
        }
    except Exception as e:
        return {"status": "error", "message": f"Error validating policy file: {str(e)}"}

# Initialize Azure OpenAI LLM for policy validation
policy_validation_llm = AzureChatOpenAI(
    azure_endpoint=AZURE_ENDPOINT,
    api_key=API_KEY,
    api_version=LLM_CONFIG["API_VERSION"],
    deployment_name=DEPLOYMENT_NAME,
    temperature=0.1,  # Low temperature for consistent analysis
    max_tokens=LLM_CONFIG["MAX_TOKENS"]
)

def convert_policy_rules_to_string(policy_rules: dict) -> str:
    """
    Converts policy rules to a string.
    
    Args:
        policy_rules: Dictionary of policy rules or list of rules.
        
    Returns:
        String representation of policy rules.
    """
    results_text = []
    
    # Add header
    results_text.append("Policy Rules Summary:")
    results_text.append("=" * 50)
    
    # Handle both dictionary with 'rules' key and direct list of rules
    rules_to_process = []
    
    if isinstance(policy_rules, list):
        # If policy_rules is already a list, use it directly
        rules_to_process = policy_rules
    elif isinstance(policy_rules, dict) and "rules" in policy_rules:
        # If policy_rules is a dict with 'rules' key, use that list
        rules_to_process = policy_rules["rules"]
    else:
        return "No valid policy rules provided"
    
    # Iterate over all rules
    for rule in rules_to_process:
        if rule["type"] == "info" and "message" in rule:
            # Add the rule message
            results_text.append(rule["message"])
            
            # Add role information
            if "role" in rule and rule["role"]:
                roles = [r.get("role", r.get("value", "Unknown")) for r in rule["role"]]
                results_text.append(f"Roles: {', '.join(roles)}")
            
            # Add resource information
            if "resource" in rule and rule["resource"]:
                resources = [r.get("value", "Unknown") for r in rule["resource"]]
                results_text.append(f"Resources: {', '.join(resources)}")
            
            # Add action information
            if "action" in rule and rule["action"]:
                actions = [a.get("value", "Unknown") for a in rule["action"]]
                results_text.append(f"Actions: {', '.join(actions)}")
            
            # Add separator between rules
            results_text.append("-" * 50)
    return "\n".join(results_text)




def parse_llm_json_response(response: str) -> Dict[str, Any]:
    """Parse JSON response from LLM, handling markdown code blocks.
    
    Args:
        response: Raw response string from LLM
        
    Returns:
        Parsed JSON data as a dictionary
        
    Raises:
        json.JSONDecodeError: If response cannot be parsed as JSON
    """
    # Clean up response if it contains markdown code block formatting
    response = response.strip()
    if response.startswith('```'):
        # Extract JSON from markdown code block
        # Remove opening markdown code block
        if '```json' in response:
            response = response.split('```json', 1)[1]
        elif '```' in response:
            response = response.split('```', 1)[1]
        # Remove closing markdown code block if present
        if '```' in response:
            response = response.split('```', 1)[0]
        response = response.strip()
    
    # Parse and return the JSON data
    return json.loads(response)

def validate_policy_with_llm(query: str, rules: str) -> dict:
    """Validate policy rules using Azure OpenAI LLM.
    
    Args:
        query: The user query for policy validation
        rules: The validation results from policy validation as string
        
    Returns:
        Dictionary with LLM analysis of the policy validation results and status:
        - "success": All rules are in match, none in deviation
        - "warning": Some rules in match, some in deviation
        - "error": No rules in match, all in deviation or no rules found
    """
    try:
        # Create system prompt for policy validation
        system_message = LLM_PROMPTS.get("POLICY_VALIDATION_SYSTEM_MESSAGE", "")
        
        # Create user prompt with validation results and query
        user_message = f"""
        Analyze the following policy rules based on this query: "{query}"
        
        Rules:
        {rules}
        
        Provide your analysis in the following JSON format:
        {{
           "query": "The user query for rules and authorization",
           "match": "List of rules that matches the query",
           "deviation": "List of rules that does not match the query"
        }}        
        """
        
        # Create the prompt template
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content=system_message),
            HumanMessage(content=user_message)
        ])
        
        # Create the chain
        chain = prompt | policy_validation_llm | StrOutputParser()
        
        # Execute the chain
        response = chain.invoke({})
        
        # Parse the LLM response
        try:
            result = parse_llm_json_response(response)
            
            # Determine validation status based on match and deviation content
            match_content = result.get("match", "")
            deviation_content = result.get("deviation", "")
            
            # Check if match is empty or indicates no matches
            match_empty = not match_content
            
            # Check if deviation is empty or indicates no deviations
            deviation_empty = not deviation_content
            
            if match_empty:
                # No rules match the query - this is an error
                status = "error"
                message = "No rules match the user requirements. Policy changes are needed."
            elif not deviation_empty:
                # Some rules match but there are deviations - this is a warning
                status = "warning"
                message = "Some rules match the requirements, but there are deviations that should be addressed."
            else:
                # All rules match and no deviations - this is success
                status = "success"
                message = "All rules match the requirements. No policy changes needed."
                
            return {
                "status": status,
                "message": message,
                "analysis": result
            }
        except json.JSONDecodeError as e:
            # If parsing fails, return the raw response
            return {
                "status": "parsing_error",
                "message": f"Failed to parse LLM response as JSON: {str(e)}",
                "raw_response": response
            }
            
    except Exception as e:
        return {"status": "error", "message": f"Error validating policy with LLM: {str(e)}"} 


if __name__ == "__main__":
    # For testing purposes
    result = policy_validation_tool("Dette er min query babygurl","/Users/johanne.norland/Documents/intro-prosjekt/konkursbo-faktiskledelseogeier/App/config/authorization/policy.xml")
    print(result)